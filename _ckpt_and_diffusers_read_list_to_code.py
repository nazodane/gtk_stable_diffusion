# Copyright 2022 Toshimitsu Kimura <lovesyao@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from pathlib import Path
home = str(Path.home())

#ckpt_file_path = home + "/.cache/huggingface/diffusers/sd-v1-4/sd-v1-4.ckpt"

from gtk_stable_diffusion.ckpt_and_diffusers_mapping import unet_ckpt_and_diffusers_mapping, vae_ckpt_and_diffusers_mapping_noconv,\
         vae_ckpt_and_diffusers_mapping_conv, text_model_ckpt_and_diffusers_mapping_noconv, text_model_ckpt_and_diffusers_mapping_conv
import torch
import diffusers
from transformers import CLIPTextModel

#ckpt_state_dict = torch.load(ckpt_file_path)["state_dict"]

code = """# This code is autogenerated code by _ckpt_and_diffusers_read_list_to_code.py

def ckpt_to_diffusers_read_list(unet, vae, text_model):
    return {
"""

unet_config = {'sample_size': 32, 'in_channels': 4, 'out_channels': 4, \
               'down_block_types': ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D'), \
               'up_block_types': ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D'), \
               'block_out_channels': (320, 640, 1280, 1280), 'layers_per_block': 2, 'cross_attention_dim': 768, 'attention_head_dim': 8}

unet = diffusers.UNet2DConditionModel(**unet_config)

vae_config = {'sample_size': 256, 'in_channels': 3, 'out_channels': 3, \
              'down_block_types': ('DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'), \
              'up_block_types': ('UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'), \
              'block_out_channels': (128, 256, 512, 512), 'latent_channels': 4, 'layers_per_block': 2}

vae = diffusers.AutoencoderKL(**vae_config)

text_model = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")

import string

def code_gen(parent_obj, parent_obj_name, mapping, code_addition = ""):
    code = ""
#    mapping = sorted(mapping, key=lambda l: l[1])
#    v_list = list(string.ascii_lowercase)
#    v_arr = []

    for (ckpt, dif) in mapping:
        path = dif.split(".")
        c = parent_obj
        code += f"""    "{ckpt}": """ + parent_obj_name
        codeline_addition = code_addition
        while len(path):
            p = path.pop(0)
            if hasattr(c, "__iter__"):
                code += f"""[{p}]"""
                c = c[int(p)]
                continue
            if p in c._modules:
                code += f"""._modules["{p}"]"""
                c = c._modules[p]
                continue
            if p in c._parameters:
                code += f"""._parameters["{p}"]"""
                c = c._parameters[p]
#                if c.dtype == torch.float16 and\
#                   ckpt_state_dict[ckpt].dtype == torch.float32:
#                    codeline_addition += ".float()"
                continue
            if p in c._buffers:
                code += f"""._buffers["{p}"]"""
                c = c._buffers[p]
                continue
            print("error: %s // code: %s // current: %s"%(dif, code, p))
            exit()
        code += ",\n" 
    return code

code += code_gen(unet, "unet", unet_ckpt_and_diffusers_mapping)
code += code_gen(vae, "vae", vae_ckpt_and_diffusers_mapping_noconv)
code += code_gen(vae, "vae", vae_ckpt_and_diffusers_mapping_conv)
code += code_gen(text_model, "text_model", text_model_ckpt_and_diffusers_mapping_noconv)
code += code_gen(text_model, "text_model", text_model_ckpt_and_diffusers_mapping_conv)
code += "}\n"
#print(code)

with open("ckpt_to_diffusers_read_list.py", "w") as f:
    f.write(code)


